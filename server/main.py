import os
import requests
import json
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional
from enum import Enum
import asyncio
from openai import OpenAI
import uvicorn
from models import system_prompt
import replicate
import fal_client
from litellm import completion

load_dotenv()

openai_api_key = os.getenv("OPENAI_API_KEY")
llama_api_key = os.getenv("LLAMA_NEMOTRON")
runway_api_key = os.getenv("RUNWAY_API_KEY")
fal_api_key = os.getenv("FAL_KEY")
replicate_api_key = os.getenv("REPLICATE_API_TOKEN")

# Configure fal client
if fal_api_key:
    fal_client.api_key = fal_api_key

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

client = OpenAI(
    base_url="https://integrate.api.nvidia.com/v1",
    api_key=llama_api_key,
)

class VideoGeneratorModel(str, Enum):
    RUNWAYML = "runwayml"
    FAL_AI = "fal_ai"
    REPLICATE = "replicate"

class VideoRequest(BaseModel):
    prompt: str
    duration: int = 5
    model: VideoGeneratorModel = VideoGeneratorModel.FAL_AI

class VideoResponse(BaseModel):
    status: str
    video_url: Optional[str] = None
    script: Optional[str] = None
    error: Optional[str] = None
    credits: str

class ImageRequest(BaseModel):
    messages: list

class ImageResponse(BaseModel):
    status: str
    image_url: Optional[str] = None
    error: Optional[str] = None
    response: Optional[dict] = None


@app.post("/scripting")
async def generate_script(prompt: str, duration: int) -> str:
    try:
        if not llama_api_key:
            # Return mock script for development
            return f"Mock script for '{prompt}': This is a demonstration script for your animated video. In production, this would be generated by AI based on your prompt."
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"Create a script for: {prompt}"}
        ]
        
        completion = client.chat.completions.create(
            model="nvidia/llama-3.3-nemotron-super-49b-v1",
            messages=messages,
            temperature=0.7,
            top_p=0.95,
            max_tokens=500,
            frequency_penalty=0,
            presence_penalty=0
        )

        script = completion.choices[0].message.content.strip()
        
        if len(script) > 500:
            script = script[:497] + "..."
            
        return script

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Script generation error: {str(e)}")

async def generate_video_with_fal_ai(prompt: str, duration: int) -> dict:
    """Generate video using Fal AI LTX Video"""
    if not fal_api_key:
        # Return mock response for development
        return {
            "status": "completed",
            "video_url": "https://commondatastorage.googleapis.com/gtv-videos-bucket/sample/BigBuckBunny.mp4",
            "credits": "Mock video for development (Fal AI API key not configured)"
        }

    try:
        clean_prompt = prompt[:500]
        
        # Submit request to Fal AI LTX Video
        handler = fal_client.submit(
            "fal-ai/ltx-video",
            arguments={
                "prompt": clean_prompt,
                "num_frames": duration * 24,  # Convert seconds to frames (24fps)
                "width": 512,
                "height": 512,
                "num_inference_steps": 20,
                "guidance_scale": 3.0,
                "enable_safety_checker": True
            }
        )
        
        # Wait for completion
        result = handler.get()
        
        if result and "video" in result and "url" in result["video"]:
            return {
                "status": "completed",
                "video_url": result["video"]["url"],
                "credits": "Video generated using Fal AI LTX Video API"
            }
        else:
            raise Exception("Invalid response format from Fal AI")

    except Exception as e:
        error_msg = f"Fal AI error: {str(e)}"
        print(error_msg)
        raise HTTPException(status_code=500, detail=error_msg)

async def generate_runway_video(prompt: str, duration: int) -> dict:
    """Generate video using RunwayML API"""
    if not runway_api_key:
        raise ValueError("RunwayML API key not found")

    num_frames = duration * 24

    headers = {
        "Authorization": f"Bearer {runway_api_key}",
        "Content-Type": "application/json"
    }

    clean_prompt = prompt[:500].replace("[", "").replace("]", "")
    
    data = {
        "model": "gen-4",
        "prompt": clean_prompt,
        "num_frames": num_frames,
        "fps": 24,
        "negative_prompt": "Bad quality, blurry, low resolution, disfigured, distorted, ugly, deformed",
        "guidance_scale": 7.5,
        "num_inference_steps": 50,
        "seed": None
    }

    try:
        response = requests.post(
            "https://api.runwayml.com/v1/generations",
            headers=headers,
            json=data
        )
        
        print(f"RunwayML API Response Status: {response.status_code}")
        print(f"RunwayML API Response: {response.text}")
        
        response.raise_for_status()
        job_data = response.json()
        
        job_id = job_data["id"]
        max_attempts = 30
        attempt = 0
        
        while attempt < max_attempts:
            status_response = requests.get(
                f"https://api.runwayml.com/v1/generations/{job_id}",
                headers=headers
            )
            status_response.raise_for_status()
            status_data = status_response.json()
            
            print(f"RunwayML Status Check: {status_data['status']}")
            
            if status_data["status"] == "completed":
                return {
                    "status": "completed",
                    "video_url": status_data["output"]["video_url"],
                    "credits": "Video generated using RunwayML Gen-2"
                }
            elif status_data["status"] == "failed":
                error_msg = status_data.get("error", "Unknown error")
                print(f"RunwayML Generation Failed: {error_msg}")
                return {
                    "status": "failed",
                    "error": error_msg,
                    "credits": "Video generation failed using RunwayML Gen-2"
                }
            
            await asyncio.sleep(2)
            attempt += 1
        
        return {
            "status": "failed",
            "error": "Generation timed out after 60 seconds",
            "credits": "Video generation timed out using RunwayML Gen-2"
        }

    except Exception as e:
        error_msg = f"RunwayML API error: {str(e)}"
        if hasattr(e, 'response') and hasattr(e.response, 'text'):
            error_msg += f"\nResponse: {e.response.text}"
        print(error_msg)
        raise HTTPException(status_code=500, detail=error_msg)

async def image_generation(messages: list) -> dict:
    # flow: create a dict for replicate models then use it here with litellm
    response = completion(
        model="replicate/luma/photon",
        messages = [{ "content": "Hello, how are you?","role": "user"}],
        max_tokens=20,
        temperature=0.5
    )

    return response

        

@app.post("/generate/video", response_model=VideoResponse)
async def generate_video_endpoint(request: VideoRequest):
    try:
        if request.duration not in [5, 10]:
            raise HTTPException(status_code=400, detail="Duration must be either 5 or 10 seconds")

        script = await generate_script(request.prompt, request.duration)

        if request.model == VideoGeneratorModel.FAL_AI:
            result = await generate_video_with_fal_ai(script, request.duration)
        elif request.model == VideoGeneratorModel.REPLICATE:
            result = await generate_runway_video(script, request.duration)
        else:
            raise HTTPException(status_code=400, detail="Unsupported video generation model")
        
        result["script"] = script
        
        return result

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/generate/image", response_model=ImageResponse)
async def generate_image_endpoint(request: ImageRequest):
    try:
        result = await image_generation(request.messages)
        
        # Convert the ModelResponse to a dictionary
        if hasattr(result, 'model_dump'):
            response_dict = result.model_dump()
        elif hasattr(result, 'dict'):
            response_dict = result.dict()
        else:
            # Fallback: convert to dict manually
            response_dict = {
                "id": getattr(result, 'id', None),
                "object": getattr(result, 'object', None),
                "created": getattr(result, 'created', None),
                "model": getattr(result, 'model', None),
                "choices": []
            }
            
            if hasattr(result, 'choices') and result.choices:
                for choice in result.choices:
                    choice_dict = {
                        "index": getattr(choice, 'index', 0),
                        "message": {},
                        "finish_reason": getattr(choice, 'finish_reason', None)
                    }
                    
                    if hasattr(choice, 'message'):
                        choice_dict["message"] = {
                            "role": getattr(choice.message, 'role', 'assistant'),
                            "content": getattr(choice.message, 'content', '')
                        }
                    
                    response_dict["choices"].append(choice_dict)
        
        return ImageResponse(
            status="completed",
            response=response_dict,
            image_url=None  # For now, since the current implementation doesn't return an actual image URL
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)

